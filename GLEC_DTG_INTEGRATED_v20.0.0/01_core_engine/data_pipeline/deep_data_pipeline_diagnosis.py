#!/usr/bin/env python3
"""
ì‹¬ì¸µì  ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°
ì§€ì—°ì‹œê°„ 1ì´ˆ~238ì´ˆ ë¶ˆì¼ì¹˜ ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ íŒŒì•…
"""

import requests
import subprocess
import json
import time
import os
import signal
from datetime import datetime, timedelta
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS
import psutil
import statistics
import threading

# ì„¤ì •
INFLUXDB_URL = "http://localhost:8086"
INFLUXDB_TOKEN = "glec-admin-token-123456789"
INFLUXDB_ORG = "glec"
INFLUXDB_BUCKET = "dtg_metrics"

def print_section(title):
    """ì„¹ì…˜ êµ¬ë¶„ì"""
    print(f"\n{'='*80}")
    print(f"ğŸ” {title}")
    print(f"{'='*80}")

def print_subsection(title):
    """ì„œë¸Œì„¹ì…˜ êµ¬ë¶„ì"""
    print(f"\n{'-'*60}")
    print(f"ğŸ“‹ {title}")
    print(f"{'-'*60}")

def analyze_data_timestamps():
    """ë°ì´í„° íƒ€ì„ìŠ¤íƒ¬í”„ ì‹¬ì¸µ ë¶„ì„"""
    print_subsection("ë°ì´í„° íƒ€ì„ìŠ¤íƒ¬í”„ ì¼ê´€ì„± ë¶„ì„")
    
    try:
        client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)
        query_api = client.query_api()
        
        # ìµœê·¼ 5ë¶„ê°„ ëª¨ë“  ë°ì´í„°ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ì„
        timestamp_query = f'''
        from(bucket: "{INFLUXDB_BUCKET}")
            |> range(start: -5m)
            |> keep(columns: ["_time", "highway", "vehicle_id", "_field"])
            |> limit(n: 1000)
        '''
        
        print("   ğŸ“Š ìµœê·¼ 5ë¶„ê°„ ë°ì´í„° íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜ì§‘ ì¤‘...")
        result = query_api.query(query=timestamp_query)
        
        timestamps = []
        current_time = datetime.now().astimezone()
        
        for table in result:
            for record in table.records:
                record_time = record.get_time()
                time_diff = (current_time - record_time).total_seconds()
                timestamps.append({
                    'time': record_time,
                    'delay': time_diff,
                    'highway': record.values.get('highway', 'unknown'),
                    'vehicle_id': record.values.get('vehicle_id', 'unknown'),
                    'field': record.values.get('_field', 'unknown')
                })
        
        if timestamps:
            delays = [ts['delay'] for ts in timestamps]
            
            print(f"   ğŸ“ˆ ë¶„ì„ëœ ë°ì´í„° í¬ì¸íŠ¸: {len(timestamps)}ê°œ")
            print(f"   â±ï¸ ì§€ì—°ì‹œê°„ í†µê³„:")
            print(f"      ìµœì†Œ: {min(delays):.1f}ì´ˆ")
            print(f"      ìµœëŒ€: {max(delays):.1f}ì´ˆ") 
            print(f"      í‰ê· : {statistics.mean(delays):.1f}ì´ˆ")
            print(f"      ì¤‘ê°„ê°’: {statistics.median(delays):.1f}ì´ˆ")
            
            # ì§€ì—°ì‹œê°„ ë¶„í¬ ë¶„ì„
            delay_ranges = {
                "ì‹¤ì‹œê°„ (0-5ì´ˆ)": 0,
                "ì•½ê°„ ì§€ì—° (5-30ì´ˆ)": 0,
                "ì‹¬ê°í•œ ì§€ì—° (30-300ì´ˆ)": 0,
                "ê·¹ì‹¬í•œ ì§€ì—° (300ì´ˆ+)": 0
            }
            
            for delay in delays:
                if delay <= 5:
                    delay_ranges["ì‹¤ì‹œê°„ (0-5ì´ˆ)"] += 1
                elif delay <= 30:
                    delay_ranges["ì•½ê°„ ì§€ì—° (5-30ì´ˆ)"] += 1
                elif delay <= 300:
                    delay_ranges["ì‹¬ê°í•œ ì§€ì—° (30-300ì´ˆ)"] += 1
                else:
                    delay_ranges["ê·¹ì‹¬í•œ ì§€ì—° (300ì´ˆ+)"] += 1
            
            print(f"\n   ğŸ“Š ì§€ì—°ì‹œê°„ ë¶„í¬:")
            for range_name, count in delay_ranges.items():
                percentage = (count / len(delays)) * 100
                print(f"      {range_name}: {count}ê°œ ({percentage:.1f}%)")
            
            # ê³ ì†ë„ë¡œë³„ ì§€ì—° íŒ¨í„´ ë¶„ì„
            highway_delays = {}
            for ts in timestamps:
                highway = ts['highway']
                if highway not in highway_delays:
                    highway_delays[highway] = []
                highway_delays[highway].append(ts['delay'])
            
            print(f"\n   ğŸ›£ï¸ ê³ ì†ë„ë¡œë³„ ì§€ì—° íŒ¨í„´:")
            for highway, delays in highway_delays.items():
                if delays:
                    avg_delay = statistics.mean(delays)
                    print(f"      {highway}: í‰ê·  {avg_delay:.1f}ì´ˆ ({len(delays)}ê°œ ìƒ˜í”Œ)")
            
            # ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ë°ì´í„° ì°¾ê¸°
            problem_data = [ts for ts in timestamps if ts['delay'] > 60]  # 1ë¶„ ì´ìƒ ì§€ì—°
            if problem_data:
                print(f"\n   ğŸš¨ ì‹¬ê°í•œ ì§€ì—° ë°ì´í„° ({len(problem_data)}ê°œ):")
                for i, pd in enumerate(problem_data[:5]):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    print(f"      {i+1}. {pd['highway']} | ì°¨ëŸ‰ {pd['vehicle_id']} | {pd['delay']:.0f}ì´ˆ ì§€ì—°")
            
            return delays
            
    except Exception as e:
        print(f"   âŒ íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return []
    
    finally:
        if 'client' in locals():
            client.close()

def check_simulator_data_generation():
    """ì‹œë®¬ë ˆì´í„° ë°ì´í„° ìƒì„± íŒ¨í„´ ë¶„ì„"""
    print_subsection("ì‹œë®¬ë ˆì´í„° ë°ì´í„° ìƒì„± íŒ¨í„´ ë¶„ì„")
    
    try:
        client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)
        query_api = client.query_api()
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„± ì†ë„ ì¸¡ì •
        print("   ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„± ì†ë„ ì¸¡ì • (30ì´ˆ ê°„ê²©ìœ¼ë¡œ 3íšŒ)...")
        
        measurements = []
        for i in range(3):
            # í˜„ì¬ ë ˆì½”ë“œ ìˆ˜ ì¸¡ì •
            count_query = f'''
            from(bucket: "{INFLUXDB_BUCKET}")
                |> range(start: -1m)
                |> count()
            '''
            
            result = query_api.query(query=count_query)
            current_count = sum(record.get_value() for table in result for record in table.records)
            
            measurements.append({
                'time': datetime.now(),
                'count': current_count
            })
            
            print(f"      ì¸¡ì • {i+1}: {current_count:,}ê°œ ë ˆì½”ë“œ (ìµœê·¼ 1ë¶„)")
            
            if i < 2:  # ë§ˆì§€ë§‰ ì¸¡ì •ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
                time.sleep(30)
        
        # ìƒì„± ì†ë„ ê³„ì‚°
        if len(measurements) >= 2:
            time_diff = (measurements[-1]['time'] - measurements[0]['time']).total_seconds()
            count_diff = measurements[-1]['count'] - measurements[0]['count']
            rate = count_diff / time_diff if time_diff > 0 else 0
            
            print(f"\n   ğŸ“ˆ ë°ì´í„° ìƒì„± ì†ë„:")
            print(f"      ì‹œê°„ ê°„ê²©: {time_diff:.0f}ì´ˆ")
            print(f"      ë ˆì½”ë“œ ì¦ê°€: {count_diff:,}ê°œ")
            print(f"      ìƒì„± ì†ë„: {rate:.1f} ë ˆì½”ë“œ/ì´ˆ")
            
            # ëª©í‘œ ì„±ëŠ¥ê³¼ ë¹„êµ
            target_rate = 100  # ëª©í‘œ: ì´ˆë‹¹ 100ê°œ
            performance_ratio = (rate / target_rate) * 100
            
            if performance_ratio >= 90:
                print(f"      âœ… ì„±ëŠ¥ ìƒíƒœ: ìš°ìˆ˜ ({performance_ratio:.1f}%)")
            elif performance_ratio >= 70:
                print(f"      âš ï¸ ì„±ëŠ¥ ìƒíƒœ: ë³´í†µ ({performance_ratio:.1f}%)")
            else:
                print(f"      âŒ ì„±ëŠ¥ ìƒíƒœ: ë¯¸í¡ ({performance_ratio:.1f}%)")
            
            return rate
            
    except Exception as e:
        print(f"   âŒ ì‹œë®¬ë ˆì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
        return 0
    
    finally:
        if 'client' in locals():
            client.close()

def check_influxdb_performance():
    """InfluxDB ì„±ëŠ¥ ë° ë¶€í•˜ ë¶„ì„"""
    print_subsection("InfluxDB ì„±ëŠ¥ ë° ë¶€í•˜ ë¶„ì„")
    
    try:
        # 1. InfluxDB ë©”íŠ¸ë¦­ API í˜¸ì¶œ
        print("   ğŸ“Š InfluxDB ë‚´ë¶€ ë©”íŠ¸ë¦­ ì¡°íšŒ...")
        
        metrics_response = requests.get(f"{INFLUXDB_URL}/metrics", timeout=10)
        if metrics_response.status_code == 200:
            metrics_text = metrics_response.text
            
            # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
            key_metrics = {
                'http_requests_total': 0,
                'storage_points_written_total': 0,
                'storage_wal_writes_total': 0,
                'go_memstats_heap_inuse_bytes': 0
            }
            
            for line in metrics_text.split('\n'):
                for metric_name in key_metrics.keys():
                    if line.startswith(metric_name) and not line.startswith('#'):
                        try:
                            value = float(line.split()[-1])
                            key_metrics[metric_name] = value
                        except:
                            pass
            
            print(f"      HTTP ìš”ì²­ ì´ê³„: {key_metrics['http_requests_total']:,.0f}")
            print(f"      ì €ì¥ëœ í¬ì¸íŠ¸ ì´ê³„: {key_metrics['storage_points_written_total']:,.0f}")
            print(f"      WAL ì“°ê¸° ì´ê³„: {key_metrics['storage_wal_writes_total']:,.0f}")
            print(f"      í™ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {key_metrics['go_memstats_heap_inuse_bytes']/1024/1024:.1f} MB")
        
        # 2. ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n   âš¡ ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)
        query_api = client.query_api()
        
        test_queries = [
            {
                'name': 'ë‹¨ìˆœ ì¹´ìš´íŠ¸',
                'query': f'from(bucket: "{INFLUXDB_BUCKET}") |> range(start: -1m) |> count()',
                'complexity': 'low'
            },
            {
                'name': 'í•„í„° + ì§‘ê³„',
                'query': f'from(bucket: "{INFLUXDB_BUCKET}") |> range(start: -5m) |> filter(fn: (r) => r["_field"] == "vehicle_speed") |> mean()',
                'complexity': 'medium'
            },
            {
                'name': 'ê³ ì†ë„ë¡œë³„ ê·¸ë£¹í™”',
                'query': f'from(bucket: "{INFLUXDB_BUCKET}") |> range(start: -2m) |> filter(fn: (r) => r["_field"] == "vehicle_speed") |> group(columns: ["highway"]) |> count()',
                'complexity': 'high'
            }
        ]
        
        query_performance = {}
        
        for test in test_queries:
            start_time = time.time()
            try:
                result = query_api.query(query=test['query'])
                end_time = time.time()
                
                # ê²°ê³¼ ìˆ˜ì§‘
                result_count = 0
                for table in result:
                    result_count += len(table.records)
                
                execution_time = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ
                query_performance[test['name']] = {
                    'time_ms': execution_time,
                    'result_count': result_count,
                    'complexity': test['complexity'],
                    'status': 'success'
                }
                
                print(f"      {test['name']}: {execution_time:.1f}ms ({result_count}ê°œ ê²°ê³¼)")
                
            except Exception as e:
                query_performance[test['name']] = {
                    'time_ms': -1,
                    'result_count': 0,
                    'complexity': test['complexity'],
                    'status': f'error: {str(e)[:50]}'
                }
                print(f"      {test['name']}: âŒ ì‹¤íŒ¨ - {str(e)[:50]}")
        
        # ì„±ëŠ¥ ìš”ì•½
        successful_queries = [q for q in query_performance.values() if q['status'] == 'success']
        if successful_queries:
            avg_time = statistics.mean([q['time_ms'] for q in successful_queries])
            print(f"\n      ğŸ“ˆ ì¿¼ë¦¬ ì„±ëŠ¥ ìš”ì•½:")
            print(f"         ì„±ê³µë¥ : {len(successful_queries)}/{len(test_queries)} ({len(successful_queries)/len(test_queries)*100:.1f}%)")
            print(f"         í‰ê·  ì‹¤í–‰ì‹œê°„: {avg_time:.1f}ms")
            
            if avg_time < 500:
                print(f"         âœ… ì„±ëŠ¥: ìš°ìˆ˜")
            elif avg_time < 2000:
                print(f"         âš ï¸ ì„±ëŠ¥: ë³´í†µ")
            else:
                print(f"         âŒ ì„±ëŠ¥: ê°œì„  í•„ìš”")
        
        return query_performance
        
    except Exception as e:
        print(f"   âŒ InfluxDB ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}
    
    finally:
        if 'client' in locals():
            client.close()

def analyze_data_consistency():
    """ë°ì´í„° ì¼ê´€ì„± ë° í’ˆì§ˆ ë¶„ì„"""
    print_subsection("ë°ì´í„° ì¼ê´€ì„± ë° í’ˆì§ˆ ë¶„ì„")
    
    try:
        client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)
        query_api = client.query_api()
        
        # 1. í•„ë“œë³„ ë°ì´í„° ì™„ì„±ë„ í™•ì¸
        print("   ğŸ“Š í•„ë“œë³„ ë°ì´í„° ì™„ì„±ë„ ë¶„ì„...")
        
        fields_query = f'''
        from(bucket: "{INFLUXDB_BUCKET}")
            |> range(start: -5m)
            |> group(columns: ["_field"])
            |> count()
        '''
        
        result = query_api.query(query=fields_query)
        field_counts = {}
        
        for table in result:
            for record in table.records:
                field_name = record.values.get('_field', 'unknown')
                count = record.get_value()
                field_counts[field_name] = count
        
        if field_counts:
            total_records = sum(field_counts.values())
            print(f"      ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")
            print(f"      í•„ë“œ ì¢…ë¥˜: {len(field_counts)}ê°œ")
            
            # ìƒìœ„ 10ê°œ í•„ë“œ í‘œì‹œ
            sorted_fields = sorted(field_counts.items(), key=lambda x: x[1], reverse=True)
            print(f"      ì£¼ìš” í•„ë“œë³„ ë ˆì½”ë“œ ìˆ˜:")
            for i, (field, count) in enumerate(sorted_fields[:10]):
                percentage = (count / total_records) * 100
                print(f"         {i+1:2d}. {field:25s}: {count:6,}ê°œ ({percentage:5.1f}%)")
        
        # 2. ê³ ì†ë„ë¡œë³„ ë°ì´í„° ê· í˜• í™•ì¸
        print(f"\n   ğŸ›£ï¸ ê³ ì†ë„ë¡œë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„...")
        
        highway_query = f'''
        from(bucket: "{INFLUXDB_BUCKET}")
            |> range(start: -5m)
            |> group(columns: ["highway"])
            |> count()
        '''
        
        result = query_api.query(query=highway_query)
        highway_counts = {}
        
        for table in result:
            for record in table.records:
                highway = record.values.get('highway', 'unknown')
                count = record.get_value()
                highway_counts[highway] = count
        
        if highway_counts:
            total_highway_records = sum(highway_counts.values())
            print(f"      ê³ ì†ë„ë¡œë³„ ë°ì´í„° ë¶„í¬:")
            
            for highway, count in sorted(highway_counts.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_highway_records) * 100
                print(f"         {highway:15s}: {count:6,}ê°œ ({percentage:5.1f}%)")
            
            # ë°ì´í„° ê· í˜• í‰ê°€
            counts = list(highway_counts.values())
            if len(counts) > 1:
                std_dev = statistics.stdev(counts)
                mean_count = statistics.mean(counts)
                cv = (std_dev / mean_count) * 100  # ë³€ë™ê³„ìˆ˜
                
                print(f"\n      ë°ì´í„° ê· í˜• ë¶„ì„:")
                print(f"         í‰ê·  ë ˆì½”ë“œ ìˆ˜: {mean_count:.0f}")
                print(f"         í‘œì¤€í¸ì°¨: {std_dev:.0f}")
                print(f"         ë³€ë™ê³„ìˆ˜: {cv:.1f}%")
                
                if cv < 10:
                    print(f"         âœ… ê· í˜• ìƒíƒœ: ë§¤ìš° ì¢‹ìŒ")
                elif cv < 25:
                    print(f"         âš ï¸ ê· í˜• ìƒíƒœ: ë³´í†µ")
                else:
                    print(f"         âŒ ê· í˜• ìƒíƒœ: ê°œì„  í•„ìš”")
        
        # 3. ë°ì´í„° í’ˆì§ˆ ì´ìƒì¹˜ íƒì§€
        print(f"\n   ğŸ” ë°ì´í„° í’ˆì§ˆ ì´ìƒì¹˜ íƒì§€...")
        
        quality_checks = [
            {
                'name': 'ë¹„ì •ìƒì ì¸ ì†ë„',
                'query': f'from(bucket: "{INFLUXDB_BUCKET}") |> range(start: -5m) |> filter(fn: (r) => r["_field"] == "vehicle_speed" and (r["_value"] < 0 or r["_value"] > 200)) |> count()',
                'threshold': 'anomaly'
            },
            {
                'name': 'ë¹„ì •ìƒì ì¸ ì—°ë¹„',
                'query': f'from(bucket: "{INFLUXDB_BUCKET}") |> range(start: -5m) |> filter(fn: (r) => r["_field"] == "fuel_efficiency" and (r["_value"] < 0 or r["_value"] > 50)) |> count()',
                'threshold': 'anomaly'
            }
        ]
        
        for check in quality_checks:
            try:
                result = query_api.query(query=check['query'])
                anomaly_count = sum(record.get_value() for table in result for record in table.records)
                
                if anomaly_count > 0:
                    print(f"         âš ï¸ {check['name']}: {anomaly_count}ê°œ ì´ìƒì¹˜ ë°œê²¬")
                else:
                    print(f"         âœ… {check['name']}: ì •ìƒ ë²”ìœ„")
                    
            except Exception as e:
                print(f"         âŒ {check['name']}: ê²€ì‚¬ ì‹¤íŒ¨ - {str(e)[:30]}")
        
        return {
            'field_counts': field_counts,
            'highway_counts': highway_counts
        }
        
    except Exception as e:
        print(f"   âŒ ë°ì´í„° ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}
    
    finally:
        if 'client' in locals():
            client.close()

def suggest_optimization_solutions(analysis_results):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì í™” ì†”ë£¨ì…˜ ì œì•ˆ"""
    print_section("ìµœì í™” ì†”ë£¨ì…˜ ë° ê¶Œê³ ì‚¬í•­")
    
    # ì§€ì—°ì‹œê°„ ë¬¸ì œ í•´ê²°ë°©ì•ˆ
    delays = analysis_results.get('delays', [])
    if delays:
        avg_delay = statistics.mean(delays)
        max_delay = max(delays)
        
        print_subsection("ì§€ì—°ì‹œê°„ ë¬¸ì œ í•´ê²°ë°©ì•ˆ")
        
        if max_delay > 300:  # 5ë¶„ ì´ìƒ ì§€ì—°
            print("   ğŸš¨ ê·¹ì‹¬í•œ ì§€ì—° ë¬¸ì œ ë°œê²¬!")
            print("   ğŸ“‹ ê¶Œê³ ì‚¬í•­:")
            print("      1. ì‹œë®¬ë ˆì´í„° í”„ë¡œì„¸ìŠ¤ ì™„ì „ ì¬ì‹œì‘")
            print("      2. InfluxDB ë°°ì¹˜ ì“°ê¸° ì„¤ì • ìµœì í™”")
            print("      3. ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì‹œê°„ ì ê²€")
            print("      4. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§")
        elif avg_delay > 30:  # 30ì´ˆ ì´ìƒ í‰ê·  ì§€ì—°
            print("   âš ï¸ ì‹¬ê°í•œ ì§€ì—° ë¬¸ì œ ì¡´ì¬")
            print("   ğŸ“‹ ê¶Œê³ ì‚¬í•­:")
            print("      1. InfluxDB ì¿¼ë¦¬ ìµœì í™”")
            print("      2. ë°ì´í„° ìˆ˜ì§‘ ì£¼ê¸° ì¡°ì •")
            print("      3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ê²€")
        else:
            print("   âœ… ì§€ì—°ì‹œê°„ ëŒ€ì²´ë¡œ ì–‘í˜¸")
    
    # ì„±ëŠ¥ ìµœì í™” ê¶Œê³ 
    data_rate = analysis_results.get('data_rate', 0)
    print_subsection("ì„±ëŠ¥ ìµœì í™” ê¶Œê³ ")
    
    if data_rate < 50:  # ì´ˆë‹¹ 50ê°œ ë¯¸ë§Œ
        print("   âŒ ë°ì´í„° ìƒì„± ì†ë„ ë¯¸í¡")
        print("   ğŸ“‹ ê°œì„  ë°©ì•ˆ:")
        print("      1. ì‹œë®¬ë ˆì´í„° ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©")
        print("      2. ë°°ì¹˜ í¬ê¸° ì¦ê°€")
        print("      3. CPU ìì› í• ë‹¹ ì¦ëŒ€")
    elif data_rate < 100:  # ì´ˆë‹¹ 100ê°œ ë¯¸ë§Œ
        print("   âš ï¸ ë°ì´í„° ìƒì„± ì†ë„ ë³´í†µ")
        print("   ğŸ“‹ ê°œì„  ë°©ì•ˆ:")
        print("      1. ì»¨ì»¤ëŸ°ì‹œ ì„¤ì • ì¡°ì •")
        print("      2. ë©”ëª¨ë¦¬ ë²„í¼ í¬ê¸° ìµœì í™”")
    else:
        print("   âœ… ë°ì´í„° ìƒì„± ì†ë„ ì–‘í˜¸")
    
    # InfluxDB ìµœì í™” ê¶Œê³ 
    query_performance = analysis_results.get('query_performance', {})
    print_subsection("InfluxDB ìµœì í™” ê¶Œê³ ")
    
    successful_queries = [q for q in query_performance.values() if q['status'] == 'success']
    if successful_queries:
        avg_query_time = statistics.mean([q['time_ms'] for q in successful_queries])
        
        if avg_query_time > 2000:  # 2ì´ˆ ì´ìƒ
            print("   âŒ ì¿¼ë¦¬ ì„±ëŠ¥ ë¯¸í¡")
            print("   ğŸ“‹ ìµœì í™” ë°©ì•ˆ:")
            print("      1. ì¸ë±ìŠ¤ ì„¤ì • ìµœì í™”")
            print("      2. ìƒ¤ë“œ ì§€ì†ì‹œê°„ ì¡°ì •")
            print("      3. ì••ì¶• ì •ì±… ì¬ê²€í† ")
            print("      4. ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì¦ëŒ€")
        elif avg_query_time > 500:  # 0.5ì´ˆ ì´ìƒ
            print("   âš ï¸ ì¿¼ë¦¬ ì„±ëŠ¥ ë³´í†µ")
            print("   ğŸ“‹ ê°œì„  ë°©ì•ˆ:")
            print("      1. ì¿¼ë¦¬ íŒ¨í„´ ìµœì í™”")
            print("      2. íƒœê·¸ vs í•„ë“œ êµ¬ì¡° ì¬ê²€í† ")
        else:
            print("   âœ… ì¿¼ë¦¬ ì„±ëŠ¥ ì–‘í˜¸")
    
    # êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íš ì œì‹œ
    print_subsection("ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²° ê³„íš")
    
    print("   ğŸ¯ Phase 1: ê¸´ê¸‰ ì¡°ì¹˜ (5ë¶„ ì´ë‚´)")
    print("      1. ì‹œë®¬ë ˆì´í„° í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘")
    print("      2. InfluxDB ì—°ê²° ìƒíƒœ í™•ì¸")
    print("      3. ë©”ëª¨ë¦¬/CPU ì‚¬ìš©ë¥  ì ê²€")
    
    print("\n   ğŸ¯ Phase 2: ì„¤ì • ìµœì í™” (30ë¶„ ì´ë‚´)")
    print("      1. InfluxDB ë°°ì¹˜ í¬ê¸° ì¡°ì •")
    print("      2. ì‹œë®¬ë ˆì´í„° ë°ì´í„° ìƒì„± ì£¼ê¸° ìµœì í™”")
    print("      3. Grafana ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ ì„¤ì •")
    
    print("\n   ğŸ¯ Phase 3: êµ¬ì¡°ì  ê°œì„  (1ì‹œê°„ ì´ë‚´)")
    print("      1. ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìµœì í™”")
    print("      2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì•ŒëŒ êµ¬ì¶•")
    print("      3. ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì‹¬ì¸µì  ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë¬¸ì œ ì§„ë‹¨ ì‹œì‘")
    print("="*80)
    print(f"ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ëª©ì : ì§€ì—°ì‹œê°„ 1ì´ˆ~238ì´ˆ ë¶ˆì¼ì¹˜ ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ íŒŒì•… ë° í•´ê²°")
    
    analysis_results = {}
    
    try:
        # 1. ë°ì´í„° íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ì„
        print_section("1. ë°ì´í„° íƒ€ì„ìŠ¤íƒ¬í”„ ì‹¬ì¸µ ë¶„ì„")
        delays = analyze_data_timestamps()
        analysis_results['delays'] = delays
        
        # 2. ì‹œë®¬ë ˆì´í„° ì„±ëŠ¥ ë¶„ì„
        print_section("2. ì‹œë®¬ë ˆì´í„° ë°ì´í„° ìƒì„± ë¶„ì„")
        data_rate = check_simulator_data_generation()
        analysis_results['data_rate'] = data_rate
        
        # 3. InfluxDB ì„±ëŠ¥ ë¶„ì„
        print_section("3. InfluxDB ì„±ëŠ¥ ë° ë¶€í•˜ ë¶„ì„")
        query_performance = check_influxdb_performance()
        analysis_results['query_performance'] = query_performance
        
        # 4. ë°ì´í„° ì¼ê´€ì„± ë¶„ì„
        print_section("4. ë°ì´í„° ì¼ê´€ì„± ë° í’ˆì§ˆ ë¶„ì„")
        data_quality = analyze_data_consistency()
        analysis_results['data_quality'] = data_quality
        
        # 5. ìµœì í™” ì†”ë£¨ì…˜ ì œì•ˆ
        print_section("5. ìµœì í™” ì†”ë£¨ì…˜ ë° ê¶Œê³ ì‚¬í•­")
        suggest_optimization_solutions(analysis_results)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ë¶„ì„ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìµœì¢… ì§„ë‹¨ ë³´ê³ ì„œ
    print_section("ğŸ¯ ì‹¬ì¸µ ì§„ë‹¨ ìµœì¢… ê²°ê³¼")
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    scores = []
    
    # ì§€ì—°ì‹œê°„ ì ìˆ˜ (0-100)
    if 'delays' in analysis_results and analysis_results['delays']:
        avg_delay = statistics.mean(analysis_results['delays'])
        if avg_delay <= 5:
            delay_score = 100
        elif avg_delay <= 30:
            delay_score = 80 - (avg_delay - 5) * 2
        elif avg_delay <= 300:
            delay_score = 30 - (avg_delay - 30) * 0.1
        else:
            delay_score = 0
        scores.append(('ì§€ì—°ì‹œê°„', delay_score))
    
    # ë°ì´í„° ìƒì„± ì†ë„ ì ìˆ˜ (0-100)
    if 'data_rate' in analysis_results:
        rate = analysis_results['data_rate']
        if rate >= 100:
            rate_score = 100
        elif rate >= 50:
            rate_score = 50 + (rate - 50) * 1
        else:
            rate_score = rate * 1
        scores.append(('ë°ì´í„° ìƒì„± ì†ë„', rate_score))
    
    # ì¿¼ë¦¬ ì„±ëŠ¥ ì ìˆ˜ (0-100)
    if 'query_performance' in analysis_results:
        qp = analysis_results['query_performance']
        successful_queries = [q for q in qp.values() if q['status'] == 'success']
        if successful_queries:
            avg_time = statistics.mean([q['time_ms'] for q in successful_queries])
            if avg_time <= 500:
                query_score = 100
            elif avg_time <= 2000:
                query_score = 80 - (avg_time - 500) * 0.04
            else:
                query_score = max(0, 20 - (avg_time - 2000) * 0.01)
            scores.append(('ì¿¼ë¦¬ ì„±ëŠ¥', query_score))
    
    # ì¢…í•© ì ìˆ˜ ì¶œë ¥
    if scores:
        overall_score = statistics.mean([score for _, score in scores])
        
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¢…í•© í‰ê°€:")
        for component, score in scores:
            print(f"   {component}: {score:.1f}/100ì ")
        print(f"   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"   ì¢…í•© ì ìˆ˜: {overall_score:.1f}/100ì ")
        
        if overall_score >= 80:
            grade = "âœ… ìš°ìˆ˜"
            action = "ì •ê¸° ëª¨ë‹ˆí„°ë§ ìœ ì§€"
        elif overall_score >= 60:
            grade = "âš ï¸ ë³´í†µ"
            action = "ì„±ëŠ¥ ìµœì í™” ê¶Œì¥"
        else:
            grade = "âŒ ë¯¸í¡"
            action = "ì¦‰ì‹œ ê°œì„  ì¡°ì¹˜ í•„ìš”"
        
        print(f"   í‰ê°€ ë“±ê¸‰: {grade}")
        print(f"   ê¶Œì¥ ì¡°ì¹˜: {action}")
    
    # ë³´ê³ ì„œ íŒŒì¼ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"deep_pipeline_diagnosis_{timestamp}.json"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“ ìƒì„¸ ì§„ë‹¨ ë³´ê³ ì„œ: {report_file}")
    print(f"ğŸ”§ ë‹¤ìŒ ë‹¨ê³„: ê¶Œê³ ì‚¬í•­ ê¸°ë°˜ ìµœì í™” ì‹¤í–‰")

if __name__ == "__main__":
    main()