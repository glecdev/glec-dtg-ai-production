#!/usr/bin/env python3
"""
ë¬¼ë¦¬ ê°œì—°ì„± ê²€ì¦ ì‹œìŠ¤í…œ v1.0
ì‹¤ì‹œê°„ ë°ì´í„°ì˜ ë¬¼ë¦¬ ë²•ì¹™ ì¤€ìˆ˜ì„± ê²€ì¦ ë° ì´ìƒì¹˜ íƒì§€
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from influxdb_client import InfluxDBClient
import warnings
import json
import time
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import logging

# ì„¤ì •
INFLUXDB_URL = "http://localhost:8086"
INFLUXDB_TOKEN = "glec-admin-token-123456789"
INFLUXDB_ORG = "glec"
INFLUXDB_BUCKET = "dtg_metrics"

class PhysicsValidationEngine:
    """ë¬¼ë¦¬ ë²•ì¹™ ê¸°ë°˜ ë°ì´í„° ê²€ì¦ ì—”ì§„"""
    
    def __init__(self):
        self.validation_rules = {
            'speed_acceleration': {
                'name': 'ì†ë„-ê°€ì†ë„ ì¼ê´€ì„±',
                'description': 'v = u + at ë²•ì¹™ ê²€ì¦',
                'tolerance': 5.0,  # í—ˆìš© ì˜¤ì°¨ (%)
                'critical': False
            },
            'fuel_speed_correlation': {
                'name': 'ì—°ë¹„-ì†ë„ ìƒê´€ê´€ê³„',
                'description': 'ì†ë„ì™€ ì—°ë¹„ì˜ ë¬¼ë¦¬ì  ê´€ê³„ ê²€ì¦',
                'tolerance': 15.0,
                'critical': True
            },
            'weight_acceleration': {
                'name': 'ì¤‘ëŸ‰-ê°€ì†ë„ ê´€ê³„',
                'description': 'F = ma ê¸°ë°˜ ì¤‘ëŸ‰ê³¼ ê°€ì†ë„ ê´€ê³„',
                'tolerance': 10.0,
                'critical': True
            },
            'co2_fuel_consistency': {
                'name': 'CO2-ì—°ë£Œì†Œëª¨ ì¼ì¹˜ì„±',
                'description': 'ì—°ë£Œì†Œëª¨ëŸ‰ê³¼ CO2 ë°°ì¶œëŸ‰ ë¹„ë¡€ ê´€ê³„',
                'tolerance': 8.0,
                'critical': True
            },
            'speed_rpm_correlation': {
                'name': 'ì†ë„-RPM ìƒê´€ê´€ê³„',
                'description': 'ì°¨ëŸ‰ ì†ë„ì™€ ì—”ì§„ RPMì˜ ê¸°ê³„ì  ê´€ê³„',
                'tolerance': 12.0,
                'critical': False
            },
            'temperature_performance': {
                'name': 'ì˜¨ë„-ì„±ëŠ¥ ìƒê´€ê´€ê³„',
                'description': 'ì—”ì§„ì˜¨ë„ì™€ ì„±ëŠ¥ì§€í‘œì˜ ì—´ì—­í•™ì  ê´€ê³„',
                'tolerance': 20.0,
                'critical': False
            }
        }
        
        # ë¬¼ë¦¬ì  ìƒìˆ˜ ë° ê¸°ì¤€ê°’
        self.physical_constants = {
            'co2_per_liter_diesel': 2.68,  # kg CO2/L ë””ì ¤
            'truck_mass_range': (5000, 40000),  # kg (5í†¤-40í†¤)
            'max_acceleration': 3.0,  # m/sÂ² (í™”ë¬¼ì°¨ ìµœëŒ€ ê°€ì†ë„)
            'optimal_speed_range': (70, 90),  # km/h (ì—°ë¹„ ìµœì  ì†ë„)
            'rpm_speed_ratio_range': (25, 45)  # RPMë‹¹ km/h
        }
        
        self.anomaly_detector = IsolationForest(
            contamination=0.1,  # 10% ì´ìƒì¹˜ë¡œ ê°€ì •
            random_state=42
        )
        self.scaler = StandardScaler()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def validate_speed_acceleration_consistency(self, data_df):
        """ì†ë„-ê°€ì†ë„ ì¼ê´€ì„± ê²€ì¦"""
        results = {
            'rule_name': 'speed_acceleration',
            'total_records': len(data_df),
            'violations': 0,
            'violation_rate': 0.0,
            'details': []
        }
        
        if 'vehicle_speed' not in data_df.columns or 'acceleration' not in data_df.columns:
            results['error'] = 'Required fields missing: vehicle_speed, acceleration'
            return results
        
        # ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤í•œ ì†ë„ ë³€í™”ëŸ‰ ê³„ì‚° (1ì´ˆ ê°„ê²© ê°€ì •)
        dt = 1.0  # seconds
        tolerance = self.validation_rules['speed_acceleration']['tolerance']
        
        violations = []
        for i in range(1, len(data_df)):
            current_speed = data_df.iloc[i]['vehicle_speed']
            prev_speed = data_df.iloc[i-1]['vehicle_speed']
            acceleration = data_df.iloc[i]['acceleration']
            
            # ì˜ˆìƒ ì†ë„ ë³€í™”ëŸ‰ (v = u + at)
            expected_speed_change = acceleration * dt * 3.6  # m/sÂ² to km/h conversion
            actual_speed_change = current_speed - prev_speed
            
            # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°
            if abs(expected_speed_change) > 0.1:  # ìµœì†Œ ì„ê³„ê°’
                relative_error = abs(actual_speed_change - expected_speed_change) / abs(expected_speed_change) * 100
                
                if relative_error > tolerance:
                    violations.append({
                        'index': i,
                        'expected_change': expected_speed_change,
                        'actual_change': actual_speed_change,
                        'error_percent': relative_error,
                        'current_speed': current_speed,
                        'acceleration': acceleration
                    })
        
        results['violations'] = len(violations)
        results['violation_rate'] = len(violations) / max(1, len(data_df) - 1) * 100
        results['details'] = violations[:10]  # ìµœëŒ€ 10ê°œë§Œ ì €ì¥
        
        return results

    def validate_fuel_speed_correlation(self, data_df):
        """ì—°ë¹„-ì†ë„ ìƒê´€ê´€ê³„ ê²€ì¦"""
        results = {
            'rule_name': 'fuel_speed_correlation',
            'total_records': len(data_df),
            'violations': 0,
            'violation_rate': 0.0,
            'correlation_coefficient': 0.0,
            'details': []
        }
        
        required_fields = ['vehicle_speed', 'fuel_efficiency_kmpl']
        if not all(field in data_df.columns for field in required_fields):
            results['error'] = f'Required fields missing: {required_fields}'
            return results
        
        # ìœ íš¨í•œ ë°ì´í„° í•„í„°ë§
        valid_data = data_df[
            (data_df['vehicle_speed'] > 0) & 
            (data_df['fuel_efficiency_kmpl'] > 0) &
            (data_df['vehicle_speed'] < 200) &  # 200km/h ì´í•˜
            (data_df['fuel_efficiency_kmpl'] < 50)  # 50km/L ì´í•˜
        ].copy()
        
        if len(valid_data) < 10:
            results['error'] = 'Insufficient valid data for correlation analysis'
            return results
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation = valid_data['vehicle_speed'].corr(valid_data['fuel_efficiency_kmpl'])
        results['correlation_coefficient'] = correlation
        
        # ì—°ë¹„-ì†ë„ ê³¡ì„  ëª¨ë¸ë§ (2ì°¨ í•¨ìˆ˜ - ë¬¼ë¦¬ì  íŠ¹ì„±)
        speeds = valid_data['vehicle_speed']
        fuel_effs = valid_data['fuel_efficiency_kmpl']
        
        # 2ì°¨ ë‹¤í•­ì‹ í”¼íŒ…
        try:
            coeffs = np.polyfit(speeds, fuel_effs, 2)
            poly_func = np.poly1d(coeffs)
            
            # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ
            predicted_fuel_effs = poly_func(speeds)
            errors = np.abs(fuel_effs - predicted_fuel_effs)
            relative_errors = (errors / fuel_effs) * 100
            
            tolerance = self.validation_rules['fuel_speed_correlation']['tolerance']
            violation_mask = relative_errors > tolerance
            
            violations = []
            violation_indices = np.where(violation_mask)[0]
            for idx in violation_indices[:10]:  # ìµœëŒ€ 10ê°œ
                violations.append({
                    'index': int(idx),
                    'speed': float(speeds.iloc[idx]),
                    'actual_fuel_eff': float(fuel_effs.iloc[idx]),
                    'predicted_fuel_eff': float(predicted_fuel_effs[idx]),
                    'error_percent': float(relative_errors.iloc[idx])
                })
            
            results['violations'] = int(violation_mask.sum())
            results['violation_rate'] = float(violation_mask.mean() * 100)
            results['details'] = violations
            
        except Exception as e:
            results['error'] = f'Curve fitting failed: {str(e)}'
        
        return results

    def validate_weight_acceleration_relationship(self, data_df):
        """ì¤‘ëŸ‰-ê°€ì†ë„ ê´€ê³„ ê²€ì¦ (F = ma)"""
        results = {
            'rule_name': 'weight_acceleration',
            'total_records': len(data_df),
            'violations': 0,
            'violation_rate': 0.0,
            'details': []
        }
        
        required_fields = ['total_weight', 'acceleration']
        if not all(field in data_df.columns for field in required_fields):
            results['error'] = f'Required fields missing: {required_fields}'
            return results
        
        # ìœ íš¨í•œ ë°ì´í„° í•„í„°ë§
        valid_data = data_df[
            (data_df['total_weight'] > 0) &
            (data_df['acceleration'].abs() > 0.1)  # ìµœì†Œ ê°€ì†ë„ ì„ê³„ê°’
        ].copy()
        
        if len(valid_data) < 5:
            results['error'] = 'Insufficient valid data'
            return results
        
        # ë¬¼ë¦¬ì  ê¸°ëŒ€ê°’ ê³„ì‚°
        # ë¬´ê±°ìš´ íŠ¸ëŸ­ì¼ìˆ˜ë¡ ê°€ì†ë„ê°€ ë‚®ì•„ì•¼ í•¨ (ì—”ì§„ ì¶œë ¥ í•œê³„)
        weights = valid_data['total_weight']
        accelerations = valid_data['acceleration'].abs()
        
        # ì¤‘ëŸ‰-ê°€ì†ë„ ë°˜ë¹„ë¡€ ê´€ê³„ ê²€ì¦
        expected_acc_factor = 1.0 / (weights / 10000)  # 10í†¤ ê¸°ì¤€ ì •ê·œí™”
        
        violations = []
        tolerance = self.validation_rules['weight_acceleration']['tolerance']
        
        for i, (weight, acceleration) in enumerate(zip(weights, accelerations)):
            # ì˜ˆìƒ ìµœëŒ€ ê°€ì†ë„ (ì¤‘ëŸ‰ ê¸°ë°˜)
            max_expected_acc = self.physical_constants['max_acceleration'] * (10000 / weight)
            
            if acceleration > max_expected_acc * (1 + tolerance/100):
                violations.append({
                    'index': i,
                    'weight': float(weight),
                    'acceleration': float(acceleration),
                    'max_expected': float(max_expected_acc),
                    'violation_ratio': float(acceleration / max_expected_acc)
                })
        
        results['violations'] = len(violations)
        results['violation_rate'] = len(violations) / len(valid_data) * 100
        results['details'] = violations[:10]
        
        return results

    def validate_co2_fuel_consistency(self, data_df):
        """CO2-ì—°ë£Œì†Œëª¨ ì¼ì¹˜ì„± ê²€ì¦"""
        results = {
            'rule_name': 'co2_fuel_consistency',
            'total_records': len(data_df),
            'violations': 0,
            'violation_rate': 0.0,
            'details': []
        }
        
        required_fields = ['co2_emission', 'fuel_efficiency_kmpl', 'vehicle_speed']
        if not all(field in data_df.columns for field in required_fields):
            results['error'] = f'Required fields missing: {required_fields}'
            return results
        
        # ìœ íš¨í•œ ë°ì´í„° í•„í„°ë§
        valid_data = data_df[
            (data_df['co2_emission'] > 0) &
            (data_df['fuel_efficiency_kmpl'] > 0) &
            (data_df['vehicle_speed'] > 0)
        ].copy()
        
        if len(valid_data) < 5:
            results['error'] = 'Insufficient valid data'
            return results
        
        # CO2 ë°°ì¶œëŸ‰ ê³„ì‚° (g/km)
        # ì—°ë£Œì†Œëª¨ëŸ‰(L/km) = 1 / fuel_efficiency_kmpl
        # CO2 ë°°ì¶œëŸ‰ = ì—°ë£Œì†Œëª¨ëŸ‰ Ã— CO2_per_liter
        
        co2_per_liter = self.physical_constants['co2_per_liter_diesel'] * 1000  # g/L
        tolerance = self.validation_rules['co2_fuel_consistency']['tolerance']
        
        violations = []
        
        for i, row in valid_data.iterrows():
            fuel_consumption_per_km = 1.0 / row['fuel_efficiency_kmpl']  # L/km
            expected_co2_per_km = fuel_consumption_per_km * co2_per_liter  # g/km
            actual_co2 = row['co2_emission']
            
            # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°
            relative_error = abs(actual_co2 - expected_co2_per_km) / expected_co2_per_km * 100
            
            if relative_error > tolerance:
                violations.append({
                    'index': int(i),
                    'actual_co2': float(actual_co2),
                    'expected_co2': float(expected_co2_per_km),
                    'error_percent': float(relative_error),
                    'fuel_efficiency': float(row['fuel_efficiency_kmpl']),
                    'speed': float(row['vehicle_speed'])
                })
        
        results['violations'] = len(violations)
        results['violation_rate'] = len(violations) / len(valid_data) * 100
        results['details'] = violations[:10]
        
        return results

    def detect_anomalies_multivariate(self, data_df):
        """ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€"""
        results = {
            'method': 'isolation_forest',
            'total_records': len(data_df),
            'anomalies': 0,
            'anomaly_rate': 0.0,
            'details': []
        }
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_columns = data_df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numeric_columns) < 3:
            results['error'] = 'Insufficient numeric columns for multivariate analysis'
            return results
        
        # í•µì‹¬ ë¬¼ë¦¬ ë³€ìˆ˜ë“¤ ìš°ì„  ì„ íƒ
        priority_columns = [
            'vehicle_speed', 'acceleration', 'fuel_efficiency_kmpl', 
            'co2_emission', 'total_weight', 'safety_score'
        ]
        
        selected_columns = [col for col in priority_columns if col in numeric_columns]
        if len(selected_columns) < 3:
            selected_columns = numeric_columns[:6]  # ìµœëŒ€ 6ê°œ ì»¬ëŸ¼
        
        try:
            # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ (NaN ì œê±°)
            clean_data = data_df[selected_columns].dropna()
            
            if len(clean_data) < 10:
                results['error'] = 'Insufficient clean data for anomaly detection'
                return results
            
            # í‘œì¤€í™”
            scaled_data = self.scaler.fit_transform(clean_data)
            
            # ì´ìƒì¹˜ íƒì§€
            anomaly_labels = self.anomaly_detector.fit_predict(scaled_data)
            anomaly_scores = self.anomaly_detector.score_samples(scaled_data)
            
            # ì´ìƒì¹˜ ì¸ë±ìŠ¤ ì¶”ì¶œ
            anomaly_indices = np.where(anomaly_labels == -1)[0]
            
            # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            anomalies_detail = []
            for idx in anomaly_indices[:15]:  # ìµœëŒ€ 15ê°œ
                original_idx = clean_data.index[idx]
                anomaly_data = {
                    'index': int(original_idx),
                    'anomaly_score': float(anomaly_scores[idx]),
                    'values': {}
                }
                
                for col in selected_columns:
                    anomaly_data['values'][col] = float(clean_data.iloc[idx][col])
                
                anomalies_detail.append(anomaly_data)
            
            results['anomalies'] = len(anomaly_indices)
            results['anomaly_rate'] = len(anomaly_indices) / len(clean_data) * 100
            results['details'] = anomalies_detail
            results['selected_features'] = selected_columns
            
        except Exception as e:
            results['error'] = f'Anomaly detection failed: {str(e)}'
        
        return results

    def run_comprehensive_validation(self, data_df):
        """ì¢…í•© ë¬¼ë¦¬ ê²€ì¦ ì‹¤í–‰"""
        self.logger.info(f"Starting comprehensive physics validation on {len(data_df)} records")
        
        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'total_records': len(data_df),
            'validation_rules': {},
            'anomaly_detection': {},
            'overall_score': 0.0,
            'critical_violations': 0,
            'recommendations': []
        }
        
        # ê°œë³„ ë¬¼ë¦¬ ë²•ì¹™ ê²€ì¦
        physics_validations = [
            self.validate_speed_acceleration_consistency,
            self.validate_fuel_speed_correlation,
            self.validate_weight_acceleration_relationship,
            self.validate_co2_fuel_consistency
        ]
        
        critical_violations = 0
        total_violation_rate = 0
        valid_tests = 0
        
        for validation_func in physics_validations:
            try:
                result = validation_func(data_df)
                rule_name = result['rule_name']
                validation_results['validation_rules'][rule_name] = result
                
                if 'error' not in result:
                    valid_tests += 1
                    total_violation_rate += result['violation_rate']
                    
                    if (self.validation_rules[rule_name]['critical'] and 
                        result['violation_rate'] > 5.0):  # 5% ì´ìƒ ìœ„ë°˜ì‹œ critical
                        critical_violations += 1
                        
            except Exception as e:
                self.logger.error(f"Validation {validation_func.__name__} failed: {e}")
        
        # ì´ìƒì¹˜ íƒì§€
        try:
            anomaly_result = self.detect_anomalies_multivariate(data_df)
            validation_results['anomaly_detection'] = anomaly_result
        except Exception as e:
            self.logger.error(f"Anomaly detection failed: {e}")
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (0-100)
        if valid_tests > 0:
            avg_violation_rate = total_violation_rate / valid_tests
            validation_results['overall_score'] = max(0, 100 - avg_violation_rate)
        
        validation_results['critical_violations'] = critical_violations
        
        # ê¶Œê³ ì‚¬í•­ ìƒì„±
        recommendations = []
        if critical_violations > 0:
            recommendations.append("Critical physics violations detected - immediate system review required")
        if avg_violation_rate > 10:
            recommendations.append("High violation rate detected - sensor calibration recommended")
        if 'anomaly_detection' in validation_results and validation_results['anomaly_detection'].get('anomaly_rate', 0) > 15:
            recommendations.append("High anomaly rate - data collection system inspection needed")
        
        if not recommendations:
            recommendations.append("Physics validation passed - system operating within normal parameters")
        
        validation_results['recommendations'] = recommendations
        
        self.logger.info(f"Validation completed. Overall score: {validation_results['overall_score']:.1f}")
        
        return validation_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ ë¬¼ë¦¬ ê°œì—°ì„± ê²€ì¦ ì‹œìŠ¤í…œ v1.0 ì‹œì‘")
    print("="*80)
    print(f"ê²€ì¦ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ëª©ì : ì‹¤ì‹œê°„ DTG ë°ì´í„°ì˜ ë¬¼ë¦¬ ë²•ì¹™ ì¤€ìˆ˜ì„± ê²€ì¦")
    
    try:
        # InfluxDB ë°ì´í„° ìˆ˜ì§‘
        print("\nğŸ“Š InfluxDBì—ì„œ ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        client = InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)
        query_api = client.query_api()
        
        # ìµœê·¼ 10ë¶„ê°„ ë°ì´í„° ì¡°íšŒ
        query = f'''
        from(bucket: "{INFLUXDB_BUCKET}")
            |> range(start: -10m)
            |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
        '''
        
        result = query_api.query_data_frame(query=query)
        
        # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(result, list):
            if len(result) == 0:
                print("âŒ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            result = result[0] if len(result) == 1 else pd.concat(result, ignore_index=True)
        
        if result.empty:
            print("âŒ ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(result)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"ğŸ“‹ ì»¬ëŸ¼: {list(result.columns)}")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        # '_time' ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì œê±° (ë¶„ì„ì— ë¶ˆí•„ìš”)
        analysis_df = result.drop(columns=[col for col in ['_time', '_start', '_stop', 'table', 'result'] 
                                          if col in result.columns])
        
        # ë¬¼ë¦¬ ê²€ì¦ ì—”ì§„ ì´ˆê¸°í™” ë° ì‹¤í–‰
        print("\nğŸ”¬ ë¬¼ë¦¬ ê²€ì¦ ì—”ì§„ ì´ˆê¸°í™”...")
        physics_engine = PhysicsValidationEngine()
        
        print("ğŸ§ª ì¢…í•© ë¬¼ë¦¬ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
        validation_results = physics_engine.run_comprehensive_validation(analysis_df)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ¯ ë¬¼ë¦¬ ê°œì—°ì„± ê²€ì¦ ê²°ê³¼")
        print("="*80)
        
        print(f"ğŸ“Š ì „ì²´ ë ˆì½”ë“œ: {validation_results['total_records']:,}ê°œ")
        print(f"ğŸ† ì „ì²´ ì ìˆ˜: {validation_results['overall_score']:.1f}/100")
        print(f"ğŸš¨ Critical ìœ„ë°˜: {validation_results['critical_violations']}ê°œ")
        
        # ê°œë³„ ê²€ì¦ ê²°ê³¼
        print(f"\nğŸ“‹ ê°œë³„ ë¬¼ë¦¬ ë²•ì¹™ ê²€ì¦ ê²°ê³¼:")
        for rule_name, result in validation_results['validation_rules'].items():
            if 'error' not in result:
                status = "âœ…" if result['violation_rate'] < 5.0 else "âš ï¸" if result['violation_rate'] < 15.0 else "âŒ"
                print(f"   {status} {rule_name}: {result['violation_rate']:.1f}% ìœ„ë°˜ ({result['violations']}ê°œ)")
            else:
                print(f"   âŒ {rule_name}: ì˜¤ë¥˜ - {result['error']}")
        
        # ì´ìƒì¹˜ íƒì§€ ê²°ê³¼
        if 'anomaly_detection' in validation_results:
            anom_result = validation_results['anomaly_detection']
            if 'error' not in anom_result:
                anom_status = "âœ…" if anom_result['anomaly_rate'] < 10.0 else "âš ï¸" if anom_result['anomaly_rate'] < 20.0 else "âŒ"
                print(f"\nğŸ” ì´ìƒì¹˜ íƒì§€ ê²°ê³¼:")
                print(f"   {anom_status} ì´ìƒì¹˜ ë¹„ìœ¨: {anom_result['anomaly_rate']:.1f}% ({anom_result['anomalies']}ê°œ)")
            else:
                print(f"\nâŒ ì´ìƒì¹˜ íƒì§€ ì˜¤ë¥˜: {anom_result['error']}")
        
        # ê¶Œê³ ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œê³ ì‚¬í•­:")
        for i, rec in enumerate(validation_results['recommendations'], 1):
            print(f"   {i}. {rec}")
        
        # ë“±ê¸‰ í‰ê°€
        score = validation_results['overall_score']
        if score >= 90:
            grade = "ğŸ¥‡ EXCELLENT (ë¬¼ë¦¬ ë²•ì¹™ ì™„ë²½ ì¤€ìˆ˜)"
        elif score >= 80:
            grade = "ğŸ¥ˆ GOOD (ì–‘í˜¸í•œ ë¬¼ë¦¬ì  ì¼ê´€ì„±)"
        elif score >= 70:
            grade = "ğŸ¥‰ FAIR (ì¼ë¶€ ê°œì„  í•„ìš”)"
        else:
            grade = "ğŸ“Š POOR (ì‹œìŠ¤í…œ ì ê²€ í•„ìš”)"
        
        print(f"\nğŸ† ì¢…í•© í‰ê°€: {grade}")
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"physics_validation_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        
        client.close()
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()